# Lesson 3 â€“ ML Inference with Docker

This project demonstrates exporting a pre-trained MobileNetV2 model to TorchScript and performing inference inside Docker containers.
Two Docker images are provided: a **fat image** (with many dev tools) and a **slim image** (optimized multi-stage build).

---

## ğŸ“‚ Repository Structure

```text
lesson-3/
â”œâ”€â”€ inference.py          # Script for running inference on an image
â”œâ”€â”€ export_model.py       # Script to export pretrained model to TorchScript
â”œâ”€â”€ model.pt              # Exported TorchScript model
â”œâ”€â”€ Dockerfile.fat        # Full-size Docker image
â”œâ”€â”€ Dockerfile.slim       # Optimized multi-stage slim image
â”œâ”€â”€ install_dev_tools.sh  # Environment setup script
â”œâ”€â”€ REPORT.md             # Comparison report of fat vs slim images
â”œâ”€â”€ README.md             # Instructions (current file)
â””â”€â”€ assets/               # Folder with test images (maltipoo.jpg)
```

---

## ğŸ› ï¸ Build Images

From the repo root, run:

```bash
docker build -t mobilenet-fat -f Dockerfile.fat .
docker build -t mobilenet-slim -f Dockerfile.slim .
```

---

## ğŸ–¼ï¸ Run Inference

Test image (`maltipoo.jpg`) is inside the `assets/` folder. Then run:

```bash
docker run --rm -v "$PWD/assets:/data" mobilenet-fat --image /data/maltipoo.jpg --topk 3
docker run --rm -v "$PWD/assets:/data" mobilenet-slim --image /data/maltipoo.jpg --topk 3
```

```
